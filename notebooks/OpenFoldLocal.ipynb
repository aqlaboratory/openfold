{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenFold Local Notebook\n",
    "\n",
    "Provides the flexibility to run inference using a local installation of OpenFold with Docker, along with the convenience of visualizing results using the same plots from the OpenFold Colab Notebook.\n",
    "\n",
    "If you have access to a machine and want to do quick inference and visualize results, there are some useful things you can do with this notebook:\n",
    "\n",
    "- You can use precomputed alignments, which enables you to run inference with different model parameters to compare results.\n",
    "- Get the best model and plots using the provided utility functions.\n",
    "- Handle long-running executions.\n",
    "\n",
    "\n",
    "Of course, you can do this solely using Docker commands in the terminal, but you would need to code/adjust the Colab functions to work with data locally. This notebook gives you a head start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the notebook\n",
    "\n",
    "Start your notebook:\n",
    "\n",
    "Go to the notebook folder:\n",
    "\n",
    "`cd notebooks`\n",
    "\n",
    "Install the requirements in your env:\n",
    "\n",
    "`pip install -r utils/requirements.txt`\n",
    "\n",
    "Start your Jupyter server\n",
    "\n",
    "`jupyter lab . --ip=\"0.0.0.0\"`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage 1: Using a sequence string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.docker_commands import run_inference\n",
    "\n",
    "# For multiple sequences, separate sequences with a colon `:`\n",
    "inference_input = \"DAGAQGAAIGSPGVLSGNVVQVPVHVPVNVCGNTVSVIGLLNPAFGNTCVNA:AGETGRTGVLVTSSATNDGDSGWGRFAG\"\n",
    "\n",
    "database_dir = \"<change_me>/path\"\n",
    "model_name = \"multimer\"\n",
    "weight_set = 'AlphaFold'\n",
    "\n",
    "run_inference(inference_input, database_dir, weight_set, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage 2: Using a pre-existing fasta directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.docker_commands import run_inference\n",
    "\n",
    "# Example usage, put your .fasta file with the sequences to process in the inference dir\n",
    "inference_dir = \"<change_me>/path\"\n",
    "databases_dir = \"<change_me>/path\"\n",
    "\n",
    "\n",
    "weight_set = 'AlphaFold'\n",
    "model_name = \"multimer\"\n",
    "\n",
    "run_inference(inference_dir, databases_dir, weight_set, model_name, use_precomputed_alignments=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
